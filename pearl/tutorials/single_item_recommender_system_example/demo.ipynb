{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Installation\n",
    "If you haven't installed Pearl, please make sure you install Pearl with the following cell. Otherwise, you can skip the cell below."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%pip uninstall Pearl -y\n",
    "%rm -rf Pearl\n",
    "!git clone https://github.com/facebookresearch/Pearl.git\n",
    "%cd Pearl\n",
    "%pip install .\n",
    "%cd .."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import Modules"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pearl.neural_networks.common.value_networks import EnsembleQValueNetwork\n",
    "from pearl.replay_buffers.sequential_decision_making.bootstrap_replay_buffer import BootstrapReplayBuffer\n",
    "from pearl.policy_learners.sequential_decision_making.bootstrapped_dqn import BootstrappedDQN\n",
    "from pearl.utils.functional_utils.experimentation.set_seed import set_seed\n",
    "from pearl.action_representation_modules.identity_action_representation_module import IdentityActionRepresentationModule\n",
    "from pearl.history_summarization_modules.lstm_history_summarization_module import LSTMHistorySummarizationModule\n",
    "from pearl.policy_learners.sequential_decision_making.deep_q_learning import DeepQLearning\n",
    "from pearl.replay_buffers.sequential_decision_making.fifo_off_policy_replay_buffer import FIFOOffPolicyReplayBuffer\n",
    "from pearl.utils.functional_utils.train_and_eval.online_learning import online_learning\n",
    "from pearl.pearl_agent import PearlAgent\n",
    "from pearl.tutorials.single_item_recommender_system_example.env_model import SequenceClassificationModel\n",
    "from pearl.tutorials.single_item_recommender_system_example.env import RecEnv\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "set_seed(0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Environment\n",
    "This environment's underlying model is trained using the MIND dataset (Wu et al. 2020).\n",
    "\n",
    "Each data point:\n",
    "- A history of impressions clicked by a user\n",
    "- Each impression is represented by an 100-dim vector\n",
    "- A list of impressions and whether or not they are clicked\n",
    "\n",
    "The environment is constructed with the following setup. Note that this example is a contrived example to illustrate Pearl's usage, agent modularity and a subset of features. Not to represent a real-world environment or problem.  \n",
    "- State: a history of impressions by a user (note that we used the history of impressions of instead of clicked impressions to speed up learning in this example. Interested Pearl users can change it to history of clicked impressions with much longer episode length and samples to run the following experiments.)\n",
    "- Dynamic action space: two randomly picked impressions\n",
    "- Action: one of the two impressions\n",
    "- Reward: click\n",
    "- Reset every 20 steps.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# load environment\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SequenceClassificationModel(100).to(device)\n",
    "model.load_state_dict(torch.load(\"env_model_state_dict.pt\"))\n",
    "actions = torch.load(\"news_embedding_small.pt\")\n",
    "env = RecEnv(list(actions.values())[:100], model)\n",
    "observation, action_space = env.reset()\n",
    "\n",
    "# experiment code\n",
    "number_of_steps = 100000\n",
    "record_period = 400"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-15T03:30:32.877168800Z",
     "start_time": "2024-01-15T03:30:32.408830100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|## Vanilla DQN Agent\n",
    "Able to handle dynamic action space but not able to handle partial observability and sparse reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "kulkpFAvnOQx",
    "ExecuteTime": {
     "end_time": "2024-01-15T03:31:38.855150200Z",
     "start_time": "2024-01-15T03:30:35.899745900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 5, step 100, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 0.0\n",
      "episode 10, step 200, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 3.0\n",
      "episode 15, step 300, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 0.0\n",
      "episode 20, step 400, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 3.0\n",
      "episode 25, step 500, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 0.0\n",
      "episode 30, step 600, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 3.0\n",
      "episode 35, step 700, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 3.0\n",
      "episode 40, step 800, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 2.0\n",
      "episode 45, step 900, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 1.0\n",
      "episode 50, step 1000, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 0.0\n",
      "episode 55, step 1100, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 1.0\n",
      "episode 60, step 1200, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 5.0\n",
      "episode 65, step 1300, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 3.0\n",
      "episode 70, step 1400, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 3.0\n",
      "episode 75, step 1500, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 2.0\n",
      "episode 80, step 1600, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 1.0\n",
      "episode 85, step 1700, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 0.0\n",
      "episode 90, step 1800, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 2.0\n",
      "episode 95, step 1900, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 2.0\n",
      "episode 100, step 2000, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 1.0\n",
      "episode 105, step 2100, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 3.0\n",
      "episode 110, step 2200, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 0.0\n",
      "episode 115, step 2300, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 0.0\n",
      "episode 120, step 2400, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 4.0\n",
      "episode 125, step 2500, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 3.0\n",
      "episode 130, step 2600, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 0.0\n",
      "episode 135, step 2700, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 1.0\n",
      "episode 140, step 2800, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 1.0\n",
      "episode 145, step 2900, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 5.0\n",
      "episode 150, step 3000, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 5.0\n",
      "episode 155, step 3100, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 2.0\n",
      "episode 160, step 3200, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 4.0\n",
      "episode 165, step 3300, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 2.0\n",
      "episode 170, step 3400, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 1.0\n",
      "episode 175, step 3500, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 3.0\n",
      "episode 180, step 3600, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 2.0\n",
      "episode 185, step 3700, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 2.0\n",
      "episode 190, step 3800, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 2.0\n",
      "episode 195, step 3900, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 3.0\n",
      "episode 200, step 4000, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 3.0\n",
      "episode 205, step 4100, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 0.0\n",
      "episode 210, step 4200, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 1.0\n",
      "episode 215, step 4300, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 0.0\n",
      "episode 220, step 4400, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 2.0\n",
      "episode 225, step 4500, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 1.0\n",
      "episode 230, step 4600, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 3.0\n",
      "episode 235, step 4700, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 2.0\n",
      "episode 240, step 4800, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 3.0\n",
      "episode 245, step 4900, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 2.0\n",
      "episode 250, step 5000, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 1.0\n",
      "episode 255, step 5100, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 5.0\n",
      "episode 260, step 5200, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 3.0\n",
      "episode 265, step 5300, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 4.0\n",
      "episode 270, step 5400, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 1.0\n",
      "episode 275, step 5500, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 3.0\n",
      "episode 280, step 5600, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 2.0\n",
      "episode 285, step 5700, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 2.0\n",
      "episode 290, step 5800, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 4.0\n",
      "episode 295, step 5900, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 0.0\n",
      "episode 300, step 6000, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 2.0\n",
      "episode 305, step 6100, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 2.0\n",
      "episode 310, step 6200, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 3.0\n",
      "episode 315, step 6300, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 2.0\n",
      "episode 320, step 6400, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 4.0\n",
      "episode 325, step 6500, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 0.0\n",
      "episode 330, step 6600, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 2.0\n",
      "episode 335, step 6700, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 4.0\n",
      "episode 340, step 6800, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 0.0\n",
      "episode 345, step 6900, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 4.0\n",
      "episode 350, step 7000, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 3.0\n",
      "episode 355, step 7100, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 0.0\n",
      "episode 360, step 7200, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 0.0\n",
      "episode 365, step 7300, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 2.0\n",
      "episode 370, step 7400, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 2.0\n",
      "episode 375, step 7500, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 5.0\n",
      "episode 380, step 7600, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 1.0\n",
      "episode 385, step 7700, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 1.0\n",
      "episode 390, step 7800, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 2.0\n",
      "episode 395, step 7900, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 0.0\n",
      "episode 400, step 8000, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 1.0\n",
      "episode 405, step 8100, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 2.0\n",
      "episode 410, step 8200, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 2.0\n",
      "episode 415, step 8300, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 1.0\n",
      "episode 420, step 8400, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 0.0\n",
      "episode 425, step 8500, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 6.0\n",
      "episode 430, step 8600, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 1.0\n",
      "episode 435, step 8700, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 1.0\n",
      "episode 440, step 8800, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 1.0\n",
      "episode 445, step 8900, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 0.0\n",
      "episode 450, step 9000, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 1.0\n",
      "episode 455, step 9100, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 1.0\n",
      "episode 460, step 9200, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 3.0\n",
      "episode 465, step 9300, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 1.0\n",
      "episode 470, step 9400, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 6.0\n",
      "episode 475, step 9500, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 1.0\n",
      "episode 480, step 9600, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 2.0\n",
      "episode 485, step 9700, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 3.0\n",
      "episode 490, step 9800, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 0.0\n",
      "episode 495, step 9900, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 2.0\n",
      "episode 500, step 10000, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 2.0\n",
      "episode 505, step 10100, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 2.0\n",
      "episode 510, step 10200, agent=PearlAgent with DeepQLearning, FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 4.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 21\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;66;03m# DQN-vanilla\u001B[39;00m\n\u001B[0;32m      9\u001B[0m agent \u001B[38;5;241m=\u001B[39m PearlAgent(\n\u001B[0;32m     10\u001B[0m     policy_learner\u001B[38;5;241m=\u001B[39mDeepQLearning(\n\u001B[0;32m     11\u001B[0m         state_dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     18\u001B[0m     device_id\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m,\n\u001B[0;32m     19\u001B[0m )\n\u001B[1;32m---> 21\u001B[0m info \u001B[38;5;241m=\u001B[39m \u001B[43monline_learning\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     22\u001B[0m \u001B[43m    \u001B[49m\u001B[43magent\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43magent\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     23\u001B[0m \u001B[43m    \u001B[49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     24\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnumber_of_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnumber_of_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     25\u001B[0m \u001B[43m    \u001B[49m\u001B[43mprint_every_x_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     26\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrecord_period\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrecord_period\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     27\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlearn_after_episode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m     28\u001B[0m \u001B[43m)\u001B[49m\n\u001B[0;32m     29\u001B[0m torch\u001B[38;5;241m.\u001B[39msave(info[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreturn\u001B[39m\u001B[38;5;124m\"\u001B[39m], \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDQN-return.pt\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     30\u001B[0m plt\u001B[38;5;241m.\u001B[39mplot(record_period \u001B[38;5;241m*\u001B[39m np\u001B[38;5;241m.\u001B[39marange(\u001B[38;5;28mlen\u001B[39m(info[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreturn\u001B[39m\u001B[38;5;124m\"\u001B[39m])), info[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreturn\u001B[39m\u001B[38;5;124m\"\u001B[39m], label\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDQN\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\Documents\\Github\\Pearl\\pearl\\utils\\functional_utils\\train_and_eval\\online_learning.py:107\u001B[0m, in \u001B[0;36monline_learning\u001B[1;34m(agent, env, number_of_episodes, number_of_steps, learn_after_episode, print_every_x_episodes, print_every_x_steps, seed, record_period)\u001B[0m\n\u001B[0;32m    105\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[0;32m    106\u001B[0m old_total_steps \u001B[38;5;241m=\u001B[39m total_steps\n\u001B[1;32m--> 107\u001B[0m episode_info, episode_total_steps \u001B[38;5;241m=\u001B[39m \u001B[43mrun_episode\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    108\u001B[0m \u001B[43m    \u001B[49m\u001B[43magent\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    109\u001B[0m \u001B[43m    \u001B[49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    110\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlearn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    111\u001B[0m \u001B[43m    \u001B[49m\u001B[43mexploit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    112\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlearn_after_episode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlearn_after_episode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    113\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtotal_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mold_total_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    114\u001B[0m \u001B[43m    \u001B[49m\u001B[43mseed\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mseed\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    115\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    116\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m number_of_steps \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m episode_total_steps \u001B[38;5;241m>\u001B[39m record_period:\n\u001B[0;32m    117\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\n\u001B[0;32m    118\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn episode is longer than the report_period: episode length \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepisode_total_steps\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    119\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m, record_period \u001B[39m\u001B[38;5;132;01m{record_period}\u001B[39;00m\u001B[38;5;124m. Try using a smaller record_period.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    120\u001B[0m     )\n",
      "File \u001B[1;32m~\\Documents\\Github\\Pearl\\pearl\\utils\\functional_utils\\train_and_eval\\online_learning.py:275\u001B[0m, in \u001B[0;36mrun_episode\u001B[1;34m(agent, env, learn, exploit, learn_after_episode, total_steps, seed)\u001B[0m\n\u001B[0;32m    272\u001B[0m     episode_steps \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    274\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m learn \u001B[38;5;129;01mand\u001B[39;00m learn_after_episode:\n\u001B[1;32m--> 275\u001B[0m     \u001B[43magent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlearn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    277\u001B[0m info \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreturn\u001B[39m\u001B[38;5;124m\"\u001B[39m: cum_reward}\n\u001B[0;32m    278\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m num_risky_sa \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32m~\\Documents\\Github\\Pearl\\pearl\\pearl_agent.py:206\u001B[0m, in \u001B[0;36mPearlAgent.learn\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    205\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mlearn\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Dict[\u001B[38;5;28mstr\u001B[39m, Any]:\n\u001B[1;32m--> 206\u001B[0m     report \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpolicy_learner\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlearn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreplay_buffer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    207\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msafety_module\u001B[38;5;241m.\u001B[39mlearn(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreplay_buffer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpolicy_learner)\n\u001B[0;32m    209\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpolicy_learner\u001B[38;5;241m.\u001B[39mon_policy:\n",
      "File \u001B[1;32m~\\Documents\\Github\\Pearl\\pearl\\policy_learners\\policy_learner.py:171\u001B[0m, in \u001B[0;36mPolicyLearner.learn\u001B[1;34m(self, replay_buffer)\u001B[0m\n\u001B[0;32m    169\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(batch, TransitionBatch):\n\u001B[0;32m    170\u001B[0m     batch \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpreprocess_batch(batch)\n\u001B[1;32m--> 171\u001B[0m     single_report \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlearn_batch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    173\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m single_report\u001B[38;5;241m.\u001B[39mitems():\n\u001B[0;32m    174\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m report:\n",
      "File \u001B[1;32m~\\Documents\\Github\\Pearl\\pearl\\policy_learners\\sequential_decision_making\\deep_td_learning.py:204\u001B[0m, in \u001B[0;36mDeepTDLearning.learn_batch\u001B[1;34m(self, batch)\u001B[0m\n\u001B[0;32m    196\u001B[0m state_action_values \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_Q\u001B[38;5;241m.\u001B[39mget_q_values(\n\u001B[0;32m    197\u001B[0m     state_batch\u001B[38;5;241m=\u001B[39mstate_batch,\n\u001B[0;32m    198\u001B[0m     action_batch\u001B[38;5;241m=\u001B[39maction_batch,\n\u001B[0;32m    199\u001B[0m     curr_available_actions_batch\u001B[38;5;241m=\u001B[39mbatch\u001B[38;5;241m.\u001B[39mcurr_available_actions,\n\u001B[0;32m    200\u001B[0m )  \u001B[38;5;66;03m# for duelling, this takes care of the mean subtraction for advantage estimation\u001B[39;00m\n\u001B[0;32m    202\u001B[0m \u001B[38;5;66;03m# Compute the Bellman Target\u001B[39;00m\n\u001B[0;32m    203\u001B[0m expected_state_action_values \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m--> 204\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_next_state_values\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    205\u001B[0m     \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_discount_factor\n\u001B[0;32m    206\u001B[0m     \u001B[38;5;241m*\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m done_batch\u001B[38;5;241m.\u001B[39mfloat())\n\u001B[0;32m    207\u001B[0m ) \u001B[38;5;241m+\u001B[39m reward_batch  \u001B[38;5;66;03m# (batch_size), r + gamma * V(s)\u001B[39;00m\n\u001B[0;32m    209\u001B[0m criterion \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mMSELoss()\n\u001B[0;32m    210\u001B[0m bellman_loss \u001B[38;5;241m=\u001B[39m criterion(state_action_values, expected_state_action_values)\n",
      "File \u001B[1;32mc:\\Users\\ericz\\anaconda3\\envs\\pytorch-latest\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    112\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[0;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    114\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[1;32m--> 115\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\Github\\Pearl\\pearl\\policy_learners\\sequential_decision_making\\deep_q_learning.py:68\u001B[0m, in \u001B[0;36mDeepQLearning._get_next_state_values\u001B[1;34m(self, batch, batch_size)\u001B[0m\n\u001B[0;32m     60\u001B[0m (\n\u001B[0;32m     61\u001B[0m     next_state,\n\u001B[0;32m     62\u001B[0m     next_available_actions,\n\u001B[0;32m     63\u001B[0m     next_unavailable_actions_mask,\n\u001B[0;32m     64\u001B[0m ) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_prepare_next_state_action_batch(batch)\n\u001B[0;32m     66\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m next_available_actions \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m---> 68\u001B[0m next_state_action_values \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_Q_target\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_q_values\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     69\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnext_state\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnext_available_actions\u001B[49m\n\u001B[0;32m     70\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mview(batch_size, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m     71\u001B[0m \u001B[38;5;66;03m# (batch_size x action_space_size)\u001B[39;00m\n\u001B[0;32m     72\u001B[0m \n\u001B[0;32m     73\u001B[0m \u001B[38;5;66;03m# Make sure that unavailable actions' Q values are assigned to -inf\u001B[39;00m\n\u001B[0;32m     74\u001B[0m next_state_action_values[next_unavailable_actions_mask] \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;28mfloat\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minf\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\Documents\\Github\\Pearl\\pearl\\neural_networks\\common\\value_networks.py:263\u001B[0m, in \u001B[0;36mVanillaQValueNetwork.get_q_values\u001B[1;34m(self, state_batch, action_batch, curr_available_actions_batch)\u001B[0m\n\u001B[0;32m    256\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_q_values\u001B[39m(\n\u001B[0;32m    257\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    258\u001B[0m     state_batch: Tensor,\n\u001B[0;32m    259\u001B[0m     action_batch: Tensor,\n\u001B[0;32m    260\u001B[0m     curr_available_actions_batch: Optional[Tensor] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    261\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m    262\u001B[0m     x \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat([state_batch, action_batch], dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m--> 263\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[1;32m~\\Documents\\Github\\Pearl\\pearl\\neural_networks\\common\\value_networks.py:254\u001B[0m, in \u001B[0;36mVanillaQValueNetwork.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m    253\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 254\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mc:\\Users\\ericz\\anaconda3\\envs\\pytorch-latest\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mc:\\Users\\ericz\\anaconda3\\envs\\pytorch-latest\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001B[0m, in \u001B[0;36mSequential.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    215\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[0;32m    216\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[1;32m--> 217\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    218\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[1;32mc:\\Users\\ericz\\anaconda3\\envs\\pytorch-latest\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mc:\\Users\\ericz\\anaconda3\\envs\\pytorch-latest\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001B[0m, in \u001B[0;36mSequential.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    215\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[0;32m    216\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[1;32m--> 217\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    218\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[1;32mc:\\Users\\ericz\\anaconda3\\envs\\pytorch-latest\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mc:\\Users\\ericz\\anaconda3\\envs\\pytorch-latest\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001B[0m, in \u001B[0;36mLinear.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# create a pearl agent\n",
    "\n",
    "action_representation_module = IdentityActionRepresentationModule(\n",
    "    max_number_actions=action_space.n,\n",
    "    representation_dim=action_space.action_dim,\n",
    ")\n",
    "\n",
    "# DQN-vanilla\n",
    "agent = PearlAgent(\n",
    "    policy_learner=DeepQLearning(\n",
    "        state_dim=1,\n",
    "        action_space=action_space,\n",
    "        hidden_dims=[64, 64],\n",
    "        training_rounds=50,\n",
    "        action_representation_module=action_representation_module,\n",
    "    ),\n",
    "    replay_buffer=FIFOOffPolicyReplayBuffer(100_000),\n",
    "    device_id=-1,\n",
    ")\n",
    "\n",
    "info = online_learning(\n",
    "    agent=agent,\n",
    "    env=env,\n",
    "    number_of_steps=number_of_steps,\n",
    "    print_every_x_steps=100,\n",
    "    record_period=record_period,\n",
    "    learn_after_episode=True,\n",
    ")\n",
    "torch.save(info[\"return\"], \"DQN-return.pt\")\n",
    "plt.plot(record_period * np.arange(len(info[\"return\"])), info[\"return\"], label=\"DQN\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN Agent with LSTM history summarization module\n",
    "\n",
    "Now the DQN agent can handle partially observable environments with history summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "cDauzO74nS4c",
    "ExecuteTime": {
     "end_time": "2024-01-15T03:31:51.819823900Z",
     "start_time": "2024-01-15T03:31:45.266296700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 5, step 100, agent=PearlAgent with DeepQLearning, LSTMHistorySummarizationModule(\n",
      "  (lstm): LSTM(101, 128, num_layers=2, batch_first=True)\n",
      "), FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 1.0\n",
      "episode 10, step 200, agent=PearlAgent with DeepQLearning, LSTMHistorySummarizationModule(\n",
      "  (lstm): LSTM(101, 128, num_layers=2, batch_first=True)\n",
      "), FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 3.0\n",
      "episode 15, step 300, agent=PearlAgent with DeepQLearning, LSTMHistorySummarizationModule(\n",
      "  (lstm): LSTM(101, 128, num_layers=2, batch_first=True)\n",
      "), FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 5.0\n",
      "episode 20, step 400, agent=PearlAgent with DeepQLearning, LSTMHistorySummarizationModule(\n",
      "  (lstm): LSTM(101, 128, num_layers=2, batch_first=True)\n",
      "), FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 3.0\n",
      "episode 25, step 500, agent=PearlAgent with DeepQLearning, LSTMHistorySummarizationModule(\n",
      "  (lstm): LSTM(101, 128, num_layers=2, batch_first=True)\n",
      "), FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 3.0\n",
      "episode 30, step 600, agent=PearlAgent with DeepQLearning, LSTMHistorySummarizationModule(\n",
      "  (lstm): LSTM(101, 128, num_layers=2, batch_first=True)\n",
      "), FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 1.0\n",
      "episode 35, step 700, agent=PearlAgent with DeepQLearning, LSTMHistorySummarizationModule(\n",
      "  (lstm): LSTM(101, 128, num_layers=2, batch_first=True)\n",
      "), FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 2.0\n",
      "episode 40, step 800, agent=PearlAgent with DeepQLearning, LSTMHistorySummarizationModule(\n",
      "  (lstm): LSTM(101, 128, num_layers=2, batch_first=True)\n",
      "), FIFOOffPolicyReplayBuffer, env=<pearl.tutorials.single_item_recommender_system_example.env.RecEnv object at 0x000002162508F590>\n",
      "return: 4.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 21\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Add a LSTM history summarization module\u001B[39;00m\n\u001B[0;32m      3\u001B[0m agent \u001B[38;5;241m=\u001B[39m PearlAgent(\n\u001B[0;32m      4\u001B[0m     policy_learner\u001B[38;5;241m=\u001B[39mDeepQLearning(\n\u001B[0;32m      5\u001B[0m         state_dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m128\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     18\u001B[0m     device_id\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m,\n\u001B[0;32m     19\u001B[0m )\n\u001B[1;32m---> 21\u001B[0m info \u001B[38;5;241m=\u001B[39m \u001B[43monline_learning\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     22\u001B[0m \u001B[43m    \u001B[49m\u001B[43magent\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43magent\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     23\u001B[0m \u001B[43m    \u001B[49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     24\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnumber_of_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnumber_of_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     25\u001B[0m \u001B[43m    \u001B[49m\u001B[43mprint_every_x_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     26\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrecord_period\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrecord_period\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     27\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlearn_after_episode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m     28\u001B[0m \u001B[43m)\u001B[49m\n\u001B[0;32m     29\u001B[0m torch\u001B[38;5;241m.\u001B[39msave(info[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreturn\u001B[39m\u001B[38;5;124m\"\u001B[39m], \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDQN-LSTM-return.pt\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     30\u001B[0m plt\u001B[38;5;241m.\u001B[39mplot(record_period \u001B[38;5;241m*\u001B[39m np\u001B[38;5;241m.\u001B[39marange(\u001B[38;5;28mlen\u001B[39m(info[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreturn\u001B[39m\u001B[38;5;124m\"\u001B[39m])), info[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreturn\u001B[39m\u001B[38;5;124m\"\u001B[39m], label\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDQN-LSTM\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\Documents\\Github\\Pearl\\pearl\\utils\\functional_utils\\train_and_eval\\online_learning.py:107\u001B[0m, in \u001B[0;36monline_learning\u001B[1;34m(agent, env, number_of_episodes, number_of_steps, learn_after_episode, print_every_x_episodes, print_every_x_steps, seed, record_period)\u001B[0m\n\u001B[0;32m    105\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[0;32m    106\u001B[0m old_total_steps \u001B[38;5;241m=\u001B[39m total_steps\n\u001B[1;32m--> 107\u001B[0m episode_info, episode_total_steps \u001B[38;5;241m=\u001B[39m \u001B[43mrun_episode\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    108\u001B[0m \u001B[43m    \u001B[49m\u001B[43magent\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    109\u001B[0m \u001B[43m    \u001B[49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    110\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlearn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    111\u001B[0m \u001B[43m    \u001B[49m\u001B[43mexploit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    112\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlearn_after_episode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlearn_after_episode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    113\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtotal_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mold_total_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    114\u001B[0m \u001B[43m    \u001B[49m\u001B[43mseed\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mseed\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    115\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    116\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m number_of_steps \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m episode_total_steps \u001B[38;5;241m>\u001B[39m record_period:\n\u001B[0;32m    117\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\n\u001B[0;32m    118\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn episode is longer than the report_period: episode length \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepisode_total_steps\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    119\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m, record_period \u001B[39m\u001B[38;5;132;01m{record_period}\u001B[39;00m\u001B[38;5;124m. Try using a smaller record_period.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    120\u001B[0m     )\n",
      "File \u001B[1;32m~\\Documents\\Github\\Pearl\\pearl\\utils\\functional_utils\\train_and_eval\\online_learning.py:275\u001B[0m, in \u001B[0;36mrun_episode\u001B[1;34m(agent, env, learn, exploit, learn_after_episode, total_steps, seed)\u001B[0m\n\u001B[0;32m    272\u001B[0m     episode_steps \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    274\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m learn \u001B[38;5;129;01mand\u001B[39;00m learn_after_episode:\n\u001B[1;32m--> 275\u001B[0m     \u001B[43magent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlearn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    277\u001B[0m info \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreturn\u001B[39m\u001B[38;5;124m\"\u001B[39m: cum_reward}\n\u001B[0;32m    278\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m num_risky_sa \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32m~\\Documents\\Github\\Pearl\\pearl\\pearl_agent.py:206\u001B[0m, in \u001B[0;36mPearlAgent.learn\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    205\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mlearn\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Dict[\u001B[38;5;28mstr\u001B[39m, Any]:\n\u001B[1;32m--> 206\u001B[0m     report \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpolicy_learner\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlearn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreplay_buffer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    207\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msafety_module\u001B[38;5;241m.\u001B[39mlearn(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreplay_buffer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpolicy_learner)\n\u001B[0;32m    209\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpolicy_learner\u001B[38;5;241m.\u001B[39mon_policy:\n",
      "File \u001B[1;32m~\\Documents\\Github\\Pearl\\pearl\\policy_learners\\policy_learner.py:171\u001B[0m, in \u001B[0;36mPolicyLearner.learn\u001B[1;34m(self, replay_buffer)\u001B[0m\n\u001B[0;32m    169\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(batch, TransitionBatch):\n\u001B[0;32m    170\u001B[0m     batch \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpreprocess_batch(batch)\n\u001B[1;32m--> 171\u001B[0m     single_report \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlearn_batch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    173\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m single_report\u001B[38;5;241m.\u001B[39mitems():\n\u001B[0;32m    174\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m report:\n",
      "File \u001B[1;32m~\\Documents\\Github\\Pearl\\pearl\\policy_learners\\sequential_decision_making\\deep_td_learning.py:219\u001B[0m, in \u001B[0;36mDeepTDLearning.learn_batch\u001B[1;34m(self, batch)\u001B[0m\n\u001B[0;32m    217\u001B[0m \u001B[38;5;66;03m# Optimize the model\u001B[39;00m\n\u001B[0;32m    218\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m--> 219\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    220\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m    222\u001B[0m \u001B[38;5;66;03m# Target Network Update\u001B[39;00m\n",
      "File \u001B[1;32mc:\\Users\\ericz\\anaconda3\\envs\\pytorch-latest\\Lib\\site-packages\\torch\\_tensor.py:487\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    477\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    478\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    479\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    480\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    485\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    486\u001B[0m     )\n\u001B[1;32m--> 487\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    488\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    489\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mc:\\Users\\ericz\\anaconda3\\envs\\pytorch-latest\\Lib\\site-packages\\torch\\autograd\\__init__.py:200\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    195\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    197\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[0;32m    198\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    199\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 200\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[0;32m    201\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    202\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Add a LSTM history summarization module\n",
    "\n",
    "agent = PearlAgent(\n",
    "    policy_learner=DeepQLearning(\n",
    "        state_dim=128,\n",
    "        action_space=action_space,\n",
    "        hidden_dims=[64, 64],\n",
    "        training_rounds=50,\n",
    "        action_representation_module=action_representation_module,\n",
    "    ),\n",
    "    history_summarization_module=LSTMHistorySummarizationModule(\n",
    "        observation_dim=1,\n",
    "        action_dim=100,\n",
    "        hidden_dim=128,\n",
    "        history_length=8,\n",
    "    ),\n",
    "    replay_buffer=FIFOOffPolicyReplayBuffer(100_000),\n",
    "    device_id=-1,\n",
    ")\n",
    "\n",
    "info = online_learning(\n",
    "    agent=agent,\n",
    "    env=env,\n",
    "    number_of_steps=number_of_steps,\n",
    "    print_every_x_steps=100,\n",
    "    record_period=record_period,\n",
    "    learn_after_episode=True,\n",
    ")\n",
    "torch.save(info[\"return\"], \"DQN-LSTM-return.pt\")\n",
    "plt.plot(record_period * np.arange(len(info[\"return\"])), info[\"return\"], label=\"DQN-LSTM\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrapped DQN Agent with LSTM History Summarization\n",
    "\n",
    "Leveraging the deep exploration value-based algorithm, now the agent can achieve a better performance in a much faster way while being able to still leverage history summarization capability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_7Cpzoi3nVAw"
   },
   "outputs": [],
   "source": [
    "# Better exploration with BootstrappedDQN-LSTM\n",
    "\n",
    "agent = PearlAgent(\n",
    "    policy_learner=BootstrappedDQN(\n",
    "        q_ensemble_network=EnsembleQValueNetwork(\n",
    "            state_dim=128,\n",
    "            action_dim=100,\n",
    "            ensemble_size=10,\n",
    "            output_dim=1,\n",
    "            hidden_dims=[64, 64],\n",
    "            prior_scale=0.3,\n",
    "        ),\n",
    "        action_space=action_space,\n",
    "        training_rounds=50,\n",
    "        action_representation_module=action_representation_module,\n",
    "    ),\n",
    "    history_summarization_module=LSTMHistorySummarizationModule(\n",
    "        observation_dim=1,\n",
    "        action_dim=100,\n",
    "        hidden_dim=128,\n",
    "        history_length=8,\n",
    "    ),\n",
    "    replay_buffer=BootstrapReplayBuffer(100_000, 1.0, 10),\n",
    "    device_id=-1,\n",
    ")\n",
    "\n",
    "info = online_learning(\n",
    "    agent=agent,\n",
    "    env=env,\n",
    "    number_of_steps=number_of_steps,\n",
    "    print_every_x_steps=100,\n",
    "    record_period=record_period,\n",
    "    learn_after_episode=True,\n",
    ")\n",
    "torch.save(info[\"return\"], \"BootstrappedDQN-LSTM-return.pt\")\n",
    "plt.plot(record_period * np.arange(len(info[\"return\"])), info[\"return\"], label=\"BootstrappedDQN-LSTM\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "In this example, we illustrated Pearl's capability of dealing with dynamic action space, standard policy learning, history summarization and intelligent exploration, all in a single agent. By running the code above, you should be able to get agent performance results similar to the figure shown in pearl/tutorials/single_item_recommender_system_example/dqn+lstm+deep_explore.png.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "custom": {
   "cells": [],
   "metadata": {
    "custom": {
     "cells": [],
     "metadata": {
      "accelerator": "GPU",
      "colab": {
       "gpuType": "T4",
       "include_colab_link": true,
       "provenance": []
      },
      "fileHeader": "",
      "fileUid": "4316417e-7688-45f2-a94f-24148bfc425e",
      "isAdHoc": false,
      "kernelspec": {
       "display_name": "pearl (local)",
       "language": "python",
       "name": "pearl_local"
      },
      "language_info": {
       "name": "python"
      }
     },
     "nbformat": 4,
     "nbformat_minor": 2
    },
    "fileHeader": "",
    "fileUid": "1158a851-91bb-437e-a391-aba92448f600",
    "indentAmount": 2,
    "isAdHoc": false,
    "language_info": {
     "name": "plaintext"
    }
   },
   "nbformat": 4,
   "nbformat_minor": 2
  },
  "indentAmount": 2,
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
